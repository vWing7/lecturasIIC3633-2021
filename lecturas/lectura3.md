# Crítica: Evaluating recommendation systems

En este artículo, se describen las distintas formas en que se evaluan sistemas recomendadores, principalmente experimentos offline, estudios de usuarios y evaluación online. Adicionalmente, se plantean y desarrollan distintas métricas usadas para medir el desempeño de sistemas recomendadores.

En primer lugar, cabe destacar que por alguna razón en varias partes del artículo se encuentran 2 signos de interrogación continuos ??, no estoy seguro a que se debe esto, pero interrumpe un poco el flujo de la lectura.

Ahora, respecto al contenido del paper, cuando se habla de "serendipity", se menciona que se quiere evitar el etiquetado por parte de humanos, pero no me queda claro por qué, considero que el etiquetado puede ser de gran utilidad en ciertas áreas que usan sistemas recomendadores, por ejemplo generos de una pelicula o un videojuego, que  pueden no ser explicitamente declarados y para los cuales etiquetas aplicadas de forma colaborativa pueden servir. Un ejemplo de esto se puede ver en la plataforma de videojuegos Steam, donde los usuarios son quienes etiquetan como comunidad, de modo que no requiere un costo adicional.

Adicionalmente, un elemento sobre el que se expone a lo largo del texto es el bias, y como contrarrestarlo al medirlo. Me parece que el sistema recomendador podría además generar un bias positivo en el usuario, es decir, este le da un rating mayor al que normalmente le habría dado a un producto si lo hubiera encontrado de forma natural. Esto se podría dar dependiendo de como se presente la recomendación, por ejemplo, si se presente de la forma "ya que te gusta X, te gustará Y", es posible que Y tenga mayor valor para el usuario ahora que concientemente sabe que es similar a X. En particular, sería interesante intentar medir este tipo de bias, aunque no necesariamente es algo plausible.

Finalmente, algo con lo que no estoy de acuerdo es la utilidad que se obtiene al presentar el grado de confianza de la predicción al usuario. En primer lugar, el estudio que presenta el paper de Herlocker et al. 2000, y que el articulo del que ahora estamos hablando usa como punto a favor de mostrar grado de confianza, solo se hizo con 210 participantes, una cantidad muy baja para ser realmente representativa, adicionalmente la forma de evaluar fue principalmente a partir de cuestionarios, lo cual no garantiza la verdad, y dado que confianza tiene una connotación positiva es aun más posible que los participantes mientan para intentar satisfacer a los investigadores. Personalmente pienso que mostrar confianza puede ser poco recomendable en algunos sistemas, limitando, por ejemplo la novedad y "serendipity", ya que el usuario podría tender a irse solo por contenido "seguro". 